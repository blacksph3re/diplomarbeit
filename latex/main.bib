
@article{agarwalDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} at the {{Edge}} of the {{Statistical Precipice}}},
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
  year = {2022},
  month = jan,
  journal = {arXiv:2108.13264 [cs, stat]},
  eprint = {2108.13264},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2108.13264},
  urldate = {2022-01-15},
  abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/sph3re/.config/Zotero/storage/CT3BNFAS/Agarwal et al. - 2022 - Deep Reinforcement Learning at the Edge of the Sta.pdf;/home/sph3re/.config/Zotero/storage/CNJRXPTD/2108.html}
}

@article{andrychowiczWhatMattersOnPolicy2020,
  title = {What {{Matters In On-Policy Reinforcement Learning}}? {{A Large-Scale Empirical Study}}},
  shorttitle = {What {{Matters In On-Policy Reinforcement Learning}}?},
  author = {Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain and Bachem, Olivier},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05990 [cs, stat]},
  eprint = {2006.05990},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05990},
  urldate = {2021-11-08},
  abstract = {In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement {$>$}50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/YGR66MVJ/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf;/home/sph3re/.config/Zotero/storage/ZS2YUC8A/2006.html}
}

@article{asgharniaLoadMitigationClass2020,
  title = {Load Mitigation of a Class of 5-{{MW}} Wind Turbine with {{RBF}} Neural Network Based Fractional-Order {{PID}} Controller},
  author = {Asgharnia, A. and Jamali, A. and Shahnazi, R. and Maheri, A.},
  year = {2020},
  month = jan,
  journal = {ISA Transactions},
  volume = {96},
  pages = {272--286},
  issn = {00190578},
  doi = {10.1016/j.isatra.2019.07.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0019057819302964},
  urldate = {2020-06-30},
  abstract = {In variable-pitch wind turbines, pitch angle control is implemented to regulate the rotor speed and power production. However, mechanical loads of the wind turbines are affected by the pitch angle adjustment. To improve the performance and at the same time alleviate the mechanical loads, a gainscheduling fractional-order PID (FOPID), where a trained RBF neural network chooses its parameters is proposed. The database, which the RBF neural network is trained based on, is created via optimization of a FOPID in several wind speeds with chaotic differential evolution (CDE) algorithm. The simulation results are compared to an RBF based PID controller that is designed via the same method, a conventional gain-scheduling baseline PI controller developed by NREL, an optimal RBF based PI controller, and a FOPI controller. The simulations indicate that the RBF based FOPID improves the control performance of the benchmark wind turbine in comparison to the other controllers, while the applied loads to the structure are mitigated. To validate the performance and robustness, all controllers are implemented on FAST wind turbine simulator. The superiority of the proposed FOPID controller is depicted in comparison to the other controllers.},
  langid = {english},
  file = {/home/sph3re/Programming/inf-pm-anw/literature/ISATrans_LoadMitigation(1).pdf}
}

@misc{bareinboimCausalReinforcementLearning2020,
  title = {Causal {{Reinforcement Learning}}},
  author = {Bareinboim, Elias},
  year = {2020},
  month = dec,
  url = {https://crl.causalai.net/},
  urldate = {2022-02-08}
}

@article{barlasReviewStateArt2010,
  title = {Review of State of the Art in Smart Rotor Control Research for Wind Turbines},
  author = {Barlas, T.K. and {van Kuik}, G.A.M.},
  year = {2010},
  month = jan,
  journal = {Progress in Aerospace Sciences},
  volume = {46},
  number = {1},
  pages = {1--27},
  issn = {03760421},
  doi = {10.1016/j.paerosci.2009.08.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0376042109000293},
  urldate = {2021-12-06},
  langid = {english}
}

@article{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.06887 [cs, stat]},
  eprint = {1707.06887},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.06887},
  urldate = {2020-07-01},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/KGVPTS2A/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf;/home/sph3re/.config/Zotero/storage/J6V8HCA8/1707.html}
}

@article{berkenkampSafeModelbasedReinforcement2017,
  title = {Safe {{Model-based Reinforcement Learning}} with {{Stability Guarantees}}},
  author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P. and Krause, Andreas},
  year = {2017},
  month = nov,
  journal = {arXiv:1705.08551 [cs, stat]},
  eprint = {1705.08551},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.08551},
  urldate = {2022-02-08},
  abstract = {Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/N7H37SQJ/Berkenkamp et al. - 2017 - Safe Model-based Reinforcement Learning with Stabi.pdf;/home/sph3re/.config/Zotero/storage/J3XJANBU/1705.html}
}

@article{birMultibladeCoordinateTransformation2008,
  title = {Multiblade Coordinate Transformation and Its Application to Wind Turbine Analysis: {{Preprint}}},
  author = {Bir, G},
  year = {2008},
  month = jan,
  address = {{United States}},
  url = {https://www.osti.gov/biblio/922553},
  annotation = {Abstract Note: This paper describes the mulitblade coordinate transformation (MBC) modeling process that integrates the dynamics of individual wind turbine blades and expresses them as fixed frames.}
}

@inproceedings{blasquesMeanLoadEffects2013,
  title = {Mean Load Effects on the Fatigue Life of Offshore Wind Turbine Monopile Foundations},
  author = {Blasques, Jos{\'e} Pedro Albergaria Amaral and Natarajan, Anand},
  year = {2013}
}

@techreport{bortolottiIEAWindTCP2019,
  title = {{{IEA Wind TCP Task}} 37: {{Systems Engineering}} in {{Wind Energy}} - {{WP2}}.1 {{Reference Wind Turbines}}},
  shorttitle = {{{IEA Wind TCP Task}} 37},
  author = {Bortolotti, Pietro and Tarres, Helena C and Dykes, Katherine L and Merz, Karl and Sethuraman, Latha and Verelst, David and Zahle, Frederik},
  year = {2019},
  month = jun,
  number = {NREL/TP-5000-73492, 1529216},
  pages = {NREL/TP-5000-73492, 1529216},
  doi = {10.2172/1529216},
  url = {http://www.osti.gov/servlets/purl/1529216/},
  urldate = {2021-10-28},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/LDC2NJAA/Bortolotti et al. - 2019 - IEA Wind TCP Task 37 Systems Engineering in Wind .pdf}
}

@article{bossanyiAdvancedControllerResearch2012,
  title = {Advanced Controller Research for Multi-{{MW}} Wind Turbines in the {{UPWIND}} Project: {{Multi-MW}} Wind Turbine Control in the {{UPWIND}} Project},
  shorttitle = {Advanced Controller Research for Multi-{{MW}} Wind Turbines in the {{UPWIND}} Project},
  author = {Bossanyi, E. and Savini, B. and Iribas, M. and Hau, M. and Fischer, B. and Schlipf, D. and Engelen, T. and Rossetti, M. and Carcangiu, C. E.},
  year = {2012},
  month = jan,
  journal = {Wind Energy},
  volume = {15},
  number = {1},
  pages = {119--145},
  issn = {10954244},
  doi = {10.1002/we.523},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/we.523},
  urldate = {2021-11-11},
  langid = {english}
}

@article{bossanyiFieldTestingIndividual2009,
  title = {Field Testing of Individual Pitch Control on the {{NREL CART2}} Wind Turbine},
  author = {Bossanyi, Ervin and Wright, Alan},
  year = {2009},
  month = jan
}

@article{bossanyiFurtherLoadReductions2005,
  title = {Further Load Reductions with Individual Pitch Control},
  author = {Bossanyi, E. A.},
  year = {2005},
  month = oct,
  journal = {Wind Energy},
  volume = {8},
  number = {4},
  pages = {481--485},
  issn = {1095-4244, 1099-1824},
  doi = {10.1002/we.166},
  url = {http://doi.wiley.com/10.1002/we.166},
  urldate = {2020-12-29},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/5V5MYKSR/Bossanyi - 2005 - Further load reductions with individual pitch cont.pdf}
}

@article{bossanyiIndividualBladePitch2003,
  title = {Individual {{Blade Pitch Control}} for {{Load Reduction}}},
  author = {Bossanyi, E. A.},
  year = {2003},
  month = apr,
  journal = {Wind Energy},
  volume = {6},
  number = {2},
  pages = {119--128},
  issn = {1095-4244, 1099-1824},
  doi = {10.1002/we.76},
  url = {http://doi.wiley.com/10.1002/we.76},
  urldate = {2020-12-29},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/Y7IG4AJM/Bossanyi - 2003 - Individual Blade Pitch Control for Load Reduction.pdf}
}

@article{bossanyiWindTurbineControl2014,
  title = {Wind Turbine Control Applications of Turbine-Mounted {{LIDAR}}},
  author = {Bossanyi, E A and Kumar, A and {Hugues-Salas}, O},
  year = {2014},
  month = dec,
  journal = {Journal of Physics: Conference Series},
  volume = {555},
  pages = {012011},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/555/1/012011},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/555/1/012011},
  urldate = {2021-11-15},
  file = {/home/sph3re/.config/Zotero/storage/F53CGQ4Y/Bossanyi et al. - 2014 - Wind turbine control applications of turbine-mount.pdf}
}

@article{brockmanOpenAIGym2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.01540 [cs]},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.01540},
  urldate = {2021-10-21},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{bruntonDiscoveringGoverningEquations2016,
  title = {Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems},
  author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
  year = {2016},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {15},
  pages = {3932--3937},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1517384113},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1517384113},
  urldate = {2022-02-15},
  abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/8L8S5VCG/Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf}
}

@inproceedings{buhlCRUNCHUSERGUIDE2001,
  title = {{{CRUNCH USER}}'{{S GUIDE}}},
  author = {Buhl, Marshall L.},
  year = {2001}
}

@article{buhlOpenFAST,
  title = {{{OpenFAST}}},
  author = {Buhl, Marshall and Hayman, Greg and Jonkman, Jason and Jonkman, Bonnie and Mudafort, Rafael and Platt, Andy and Sprague, Mike},
  url = {https://github.com/OpenFAST/openfast}
}

@book{burtonWindEnergyHandbook2011,
  title = {Wind {{Energy Handbook}}: {{Burton}}/{{Wind Energy Handbook}}},
  shorttitle = {Wind {{Energy Handbook}}},
  author = {Burton, Tony and Jenkins, Nick and Sharpe, David and Bossanyi, Ervin},
  year = {2011},
  month = may,
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781119992714},
  url = {http://doi.wiley.com/10.1002/9781119992714},
  urldate = {2021-11-04},
  isbn = {978-1-119-99271-4 978-0-470-69975-1}
}

@article{chenAddressingActionOscillations2021,
  title = {Addressing {{Action Oscillations}} through {{Learning Policy Inertia}}},
  author = {Chen, Chen and Tang, Hongyao and Hao, Jianye and Liu, Wulong and Meng, Zhaopeng},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.02287 [cs]},
  eprint = {2103.02287},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.02287},
  urldate = {2022-01-30},
  abstract = {Deep reinforcement learning (DRL) algorithms have been demonstrated to be effective in a wide range of challenging decision making and control tasks. However, these methods typically suffer from severe action oscillations in particular in discrete action setting, which means that agents select different actions within consecutive steps even though states only slightly differ. This issue is often neglected since the policy is usually evaluated by its cumulative rewards only. Action oscillation strongly affects the user experience and can even cause serious potential security menace especially in real-world domains with the main concern of safety, such as autonomous driving. To this end, we introduce Policy Inertia Controller (PIC) which serves as a generic plug-in framework to off-the-shelf DRL algorithms, to enables adaptive trade-off between the optimality and smoothness of the learned policy in a formal way. We propose Nested Policy Iteration as a general training algorithm for PIC-augmented policy which ensures monotonically non-decreasing updates under some mild conditions. Further, we derive a practical DRL algorithm, namely Nested Soft Actor-Critic. Experiments on a collection of autonomous driving tasks and several Atari games suggest that our approach demonstrates substantial oscillation reduction in comparison to a range of commonly adopted baselines with almost no performance degradation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/CG8K3YZX/Chen et al. - 2021 - Addressing Action Oscillations through Learning Po.pdf;/home/sph3re/.config/Zotero/storage/GL74VF67/2103.html}
}

@article{chenReinforcementbasedRobustVariable2020,
  title = {Reinforcement-Based Robust Variable Pitch Control of Wind Turbines},
  author = {Chen, Peng and Han, Dezhi and Tan, Fuxiao and Wang, Jun},
  year = {2020},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {8},
  pages = {20493--20502},
  doi = {10.1109/ACCESS.2020.2968853}
}

@inproceedings{cobbeQuantifyingGeneralizationReinforcement2019,
  title = {Quantifying Generalization in Reinforcement Learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {1282--1289},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v97/cobbe19a.html},
  abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
  pdf = {http://proceedings.mlr.press/v97/cobbe19a/cobbe19a.pdf}
}

@misc{contributorsGarageToolkitReproducible2019,
  title = {Garage: {{A}} Toolkit for Reproducible Reinforcement Learning Research},
  author = {garage {contributors}, The},
  year = {2019},
  publisher = {{GitHub}},
  url = {https://github.com/rlworkgroup/garage},
  commit = {b4abe07f0fa9bac2cb70e4a3e315c2e7e5b08507}
}

@misc{cop21ParisAgreement2015,
  title = {The {{Paris Agreement}}},
  author = {COP21},
  year = {2015},
  month = dec,
  url = {https://unfccc.int/documents/184656}
}

@article{coqueletBiomimeticIndividualPitch2020,
  title = {Biomimetic Individual Pitch Control for Load Alleviation},
  author = {Coquelet, M and Bricteux, L and Lejeune, M and Chatelain, P},
  year = {2020},
  month = sep,
  journal = {Journal of Physics: Conference Series},
  volume = {1618},
  pages = {022052},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1618/2/022052},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1618/2/022052},
  urldate = {2021-10-18},
  file = {/home/sph3re/.config/Zotero/storage/X2MXAHQQ/Coquelet et al. - 2020 - Biomimetic individual pitch control for load allev.pdf}
}

@article{degrisOffPolicyActorCritic2013,
  title = {Off-{{Policy Actor-Critic}}},
  author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
  year = {2013},
  month = jun,
  journal = {arXiv:1205.4839 [cs]},
  eprint = {1205.4839},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1205.4839},
  urldate = {2021-11-27},
  abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/KDPCM4H8/Degris et al. - 2013 - Off-Policy Actor-Critic.pdf;/home/sph3re/.config/Zotero/storage/Q7CJ8DDZ/1205.html}
}

@article{demolliWindPowerForecasting2019,
  title = {Wind Power Forecasting Based on Daily Wind Speed Data Using Machine Learning Algorithms},
  author = {Demolli, Halil and Dokuz, Ahmet Sakir and Ecemis, Alper and Gokcek, Murat},
  year = {2019},
  month = oct,
  journal = {Energy Conversion and Management},
  volume = {198},
  pages = {111823},
  issn = {01968904},
  doi = {10.1016/j.enconman.2019.111823},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0196890419308052},
  urldate = {2021-11-15},
  langid = {english}
}

@article{duanDistributionalSoftActorCritic2020,
  title = {Distributional {{Soft Actor-Critic}}: {{Off-Policy Reinforcement Learning}} for {{Addressing Value Estimation Errors}}},
  shorttitle = {Distributional {{Soft Actor-Critic}}},
  author = {Duan, Jingliang and Guan, Yang and Li, Shengbo Eben and Ren, Yangang and Cheng, Bo},
  year = {2020},
  month = feb,
  journal = {arXiv:2001.02811 [cs, eess]},
  eprint = {2001.02811},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2001.02811},
  urldate = {2020-07-13},
  abstract = {In current reinforcement learning (RL) methods, function approximation errors are known to lead to the overestimated or underestimated Q-value estimates, thus resulting in suboptimal policies. We show that the learning of a state-action return distribution function can be used to improve the Q-value estimation accuracy. We employ the return distribution function within the maximum entropy RL framework in order to develop what we call the Distributional Soft Actor-Critic (DSAC) algorithm, which is an off-policy method for continuous control setting. Unlike traditional distributional RL algorithms which typically only learn a discrete return distribution, DSAC directly learns a continuous return distribution by truncating the difference between the target and current distribution to prevent gradient explosion. Additionally, we propose a new Parallel Asynchronous Buffer-Actor-Learner architecture (PABAL) to improve the learning efficiency, which is a generalization of current high-throughput learning architectures. We evaluate our method on the suite of MuJoCo continuous control tasks, achieving state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control}
}

@article{duanGeneralizedPolicyIteration2019,
  title = {Generalized {{Policy Iteration}} for {{Optimal Control}} in {{Continuous Time}}},
  author = {Duan, Jingliang and Li, Shengbo Eben and Liu, Zhengyu and Bujarbaruah, Monimoy and Cheng, Bo},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.05402 [cs, eess]},
  eprint = {1909.05402},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1909.05402},
  urldate = {2020-07-06},
  abstract = {This paper proposes the Deep Generalized Policy Iteration (DGPI) algorithm to find the infinite horizon optimal control policy for general nonlinear continuous-time systems with known dynamics. Unlike existing adaptive dynamic programming algorithms for continuous time systems, DGPI does not require the admissibility of initialized policy, and input-affine nature of controlled systems for convergence. Our algorithm employs the actor-critic architecture to approximate both policy and value functions with the purpose of iteratively solving the Hamilton-Jacobi-Bellman equation. Both the policy and value functions are approximated by deep neural networks. Given any arbitrary initial policy, the proposed DGPI algorithm can eventually converge to an admissible, and subsequently an optimal policy for an arbitrary nonlinear system. We also relax the update termination conditions of both the policy evaluation and improvement processes, which leads to a faster convergence speed than conventional Policy Iteration (PI) methods, for the same architecture of function approximators. We further prove the convergence and optimality of the algorithm with thorough Lyapunov analysis, and demonstrate its generality and efficacy using two detailed numerical examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/sph3re/.config/Zotero/storage/8JRQZKSV/Duan et al. - 2019 - Generalized Policy Iteration for Optimal Control i.pdf;/home/sph3re/.config/Zotero/storage/WYF8N45W/1909.html}
}

@article{ernstReinforcementLearningModel2009,
  title = {Reinforcement {{Learning Versus Model Predictive Control}}: {{A Comparison}} on a {{Power System Problem}}},
  shorttitle = {Reinforcement {{Learning Versus Model Predictive Control}}},
  author = {Ernst, D. and Glavic, M. and Capitanescu, F. and Wehenkel, L.},
  year = {2009},
  month = apr,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {39},
  number = {2},
  pages = {517--529},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2008.2007630},
  url = {http://ieeexplore.ieee.org/document/4717266/},
  urldate = {2020-07-01},
  file = {/home/sph3re/.config/Zotero/storage/IVPE2D3Y/Ernst et al. - 2009 - Reinforcement Learning Versus Model Predictive Con.pdf}
}

@inproceedings{freeburyDeterminingEquivalentDamage2000,
  title = {Determining Equivalent Damage Loading for Full-Scale Wind Turbine Blade Fatigue Tests},
  booktitle = {2000 Asme Wind Energy Symposium},
  author = {Freebury, Gregg and Musial, Walter},
  year = {2000},
  pages = {50}
}

@article{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and {van Hoof}, Herke and Meger, David},
  year = {2018},
  month = oct,
  journal = {arXiv:1802.09477 [cs, stat]},
  eprint = {1802.09477},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2021-10-18},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/A4X5M8YY/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf;/home/sph3re/.config/Zotero/storage/539I8T8L/1802.html}
}

@article{gencogluHARKSideDeep2019,
  title = {{{HARK Side}} of {{Deep Learning}} -- {{From Grad Student Descent}} to {{Automated Machine Learning}}},
  author = {Gencoglu, Oguzhan and {van Gils}, Mark and Guldogan, Esin and Morikawa, Chamin and S{\"u}zen, Mehmet and Gruber, Mathias and Leinonen, Jussi and Huttunen, Heikki},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.07633 [cs]},
  eprint = {1904.07633},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.07633},
  urldate = {2021-11-28},
  abstract = {Recent advancements in machine learning research, i.e., deep learning, introduced methods that excel conventional algorithms as well as humans in several complex tasks, ranging from detection of objects in images and speech recognition to playing difficult strategic games. However, the current methodology of machine learning research and consequently, implementations of the real-world applications of such algorithms, seems to have a recurring HARKing (Hypothesizing After the Results are Known) issue. In this work, we elaborate on the algorithmic, economic and social reasons and consequences of this phenomenon. We present examples from current common practices of conducting machine learning research (e.g. avoidance of reporting negative results) and failure of generalization ability of the proposed algorithms and datasets in actual real-life usage. Furthermore, a potential future trajectory of machine learning research and development from the perspective of accountable, unbiased, ethical and privacy-aware algorithmic decision making is discussed. We would like to emphasize that with this discussion we neither claim to provide an exhaustive argumentation nor blame any specific institution or individual on the raised issues. This is simply a discussion put forth by us, insiders of the machine learning field, reflecting on us.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/FP2WA9LV/Gencoglu et al. - 2019 - HARK Side of Deep Learning -- From Grad Student De.pdf;/home/sph3re/.config/Zotero/storage/F88EKDKE/1904.html}
}

@misc{googleProtocolBuffers,
  title = {Protocol Buffers},
  author = {{Google}},
  url = {http://code.google.com/apis/protocolbuffers/},
  urldate = {2021-12-02}
}

@article{haarnojaReinforcementLearningDeep2017,
  title = {Reinforcement {{Learning}} with {{Deep Energy-Based Policies}}},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  journal = {arXiv:1702.08165 [cs]},
  eprint = {1702.08165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1702.08165},
  urldate = {2021-11-27},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/KPJ4EDNI/Haarnoja et al. - 2017 - Reinforcement Learning with Deep Energy-Based Poli.pdf;/home/sph3re/.config/Zotero/storage/U3HKZ9JC/1702.html}
}

@article{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2019},
  month = jan,
  journal = {arXiv:1812.05905 [cs, stat]},
  eprint = {1812.05905},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.05905},
  urldate = {2021-11-10},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/5JLJB9GJ/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf;/home/sph3re/.config/Zotero/storage/R96CQY9X/1812.html}
}

@article{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  journal = {arXiv:1801.01290 [cs, stat]},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2020-06-30},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/W5BFXU29/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf;/home/sph3re/.config/Zotero/storage/4QQKDFVS/1801.html}
}

@article{hanCombinedWindTurbine2014,
  title = {Combined Wind Turbine Fatigue and Ultimate Load Reduction by Individual Blade Control},
  author = {Han, Y and Leithead, W E},
  year = {2014},
  month = jun,
  journal = {Journal of Physics: Conference Series},
  volume = {524},
  pages = {012062},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/524/1/012062},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/524/1/012062},
  urldate = {2021-11-11},
  file = {/home/sph3re/.config/Zotero/storage/6Y6CECUX/Han and Leithead - 2014 - Combined wind turbine fatigue and ultimate load re.pdf}
}

@inproceedings{hansenBasicDTUWind2013,
  title = {Basic {{DTU}} Wind Energy Controller},
  author = {Hansen, Morten H. and Henriksen, Lars Christian},
  year = {2013}
}

@inproceedings{hasseltDoubleQlearning2010,
  title = {Double Q-Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hasselt, Hado},
  editor = {Lafferty, J. and Williams, C. and {Shawe-Taylor}, J. and Zemel, R. and Culotta, A.},
  year = {2010},
  volume = {23},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf}
}

@article{haymanMLifeTheoryManual2012,
  title = {{{MLife}} Theory Manual for Version 1.00},
  author = {Hayman, G},
  year = {2012},
  journal = {National Renewable Energy Laboratory (NREL)},
  url = {https://orbit.dtu.dk/en/publications/basic-dtu-wind-energy-controller}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
  urldate = {2022-02-20},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english}
}

@article{hosseiniImprovingResponseWind2020,
  title = {Improving Response of Wind Turbines by Pitch Angle Controller Based on Gain-Scheduled Recurrent {{ANFIS}} Type 2 with Passive Reinforcement Learning},
  author = {Hosseini, Ehsan and Aghadavoodi, Ehsan and Fern{\'a}ndez Ram{\'i}rez, Luis M.},
  year = {2020},
  month = sep,
  journal = {Renewable Energy},
  volume = {157},
  pages = {897--910},
  issn = {09601481},
  doi = {10.1016/j.renene.2020.05.060},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960148120307588},
  urldate = {2021-11-08},
  langid = {english}
}

@article{howlandWindFarmPower2019,
  title = {Wind Farm Power Optimization through Wake Steering},
  author = {Howland, Michael F and Lele, Sanjiva K and Dabiri, John O},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {29},
  pages = {14495--14500},
  publisher = {{National Acad Sciences}}
}

@article{huangAdversarialAttacksNeural2017,
  title = {Adversarial Attacks on Neural Network Policies},
  author = {Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.02284},
  eprint = {1702.02284},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@book{hutterAutomatedMachineLearning2019,
  title = {Automated Machine Learning: Methods, Systems, Challenges},
  shorttitle = {Automated Machine Learning},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  series = {The {{Springer}} Series on Challenges in Machine Learning},
  publisher = {{Springer}},
  address = {{Cham, Switzerland}},
  abstract = {This open access book presents the first comprehensive overview of general methods in Automated Machine Learning (AutoML), collects descriptions of existing systems based on these methods, and discusses the first series of international challenges of AutoML systems. The recent success of commercial ML applications and the rapid growth of the field has created a high demand for off-the-shelf ML methods that can be used easily and without expert knowledge. However, many of the recent machine learning successes crucially rely on human experts, who manually select appropriate ML architectures (deep learning architectures or more traditional ML workflows) and their hyperparameters. To overcome this problem, the field of AutoML targets a progressive automation of machine learning, based on principles from optimization and machine learning itself. This book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use AutoML in their work},
  isbn = {978-3-030-05317-8},
  langid = {english}
}

@article{hwangboControlQuadrotorReinforcement2017,
  title = {Control of a Quadrotor with Reinforcement Learning},
  author = {Hwangbo, Jemin and Sa, Inkyu and Siegwart, Roland and Hutter, Marco},
  year = {2017},
  journal = {IEEE Robotics and Automation Letters},
  volume = {2},
  number = {4},
  pages = {2096--2103},
  doi = {10.1109/LRA.2017.2720851}
}

@techreport{internationalelectrotechnicalcommissionIEC61400120192019,
  title = {{{IEC}} 61400-1:2019 {{Wind}} Energy Generation Systems - {{Part}} 1: {{Design}} Requirements},
  shorttitle = {{{IEC}} 61400-1},
  author = {International Electrotechnical Commission},
  year = {2019},
  month = feb
}

@article{jonesOvercomingFundamentalLimitations2018,
  title = {Overcoming Fundamental Limitations of Wind Turbine Individual Blade Pitch Control with Inflow Sensors: {{Overcoming}} Fundamental Limitations of Wind Turbine Individual Blade Pitch Control with Inflow Sensors},
  shorttitle = {Overcoming Fundamental Limitations of Wind Turbine Individual Blade Pitch Control with Inflow Sensors},
  author = {Jones, B. Ll. and Lio, W. H. and Rossiter, J. A.},
  year = {2018},
  month = oct,
  journal = {Wind Energy},
  volume = {21},
  number = {10},
  pages = {922--936},
  issn = {10954244},
  doi = {10.1002/we.2205},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/we.2205},
  urldate = {2021-11-11},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/S2PMT2SB/Jones et al. - 2018 - Overcoming fundamental limitations of wind turbine.pdf}
}

@techreport{jonkmanDefinition5MWReference2009,
  title = {Definition of a 5-{{MW Reference Wind Turbine}} for {{Offshore System Development}}},
  author = {Jonkman, J. and Butterfield, S. and Musial, W. and Scott, G.},
  year = {2009},
  month = feb,
  number = {NREL/TP-500-38060, 947422},
  pages = {NREL/TP-500-38060, 947422},
  doi = {10.2172/947422},
  url = {http://www.osti.gov/servlets/purl/947422-nhrlni/},
  urldate = {2021-11-09},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/QQYLWIWX/Jonkman et al. - 2009 - Definition of a 5-MW Reference Wind Turbine for Of.pdf}
}

@inproceedings{kakadeNaturalPolicyGradient2001,
  title = {A Natural Policy Gradient},
  booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: {{Natural}} and Synthetic},
  author = {Kakade, Sham},
  year = {2001},
  series = {{{NIPS}}'01},
  pages = {1531--1538},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}}
}

@article{kanevWindTurbineExtreme2010,
  title = {Wind Turbine Extreme Gust Control},
  author = {Kanev, Stoyan and {van Engelen}, Tim},
  year = {2010},
  month = jan,
  journal = {Wind Energy},
  volume = {13},
  number = {1},
  pages = {18--35},
  issn = {10954244, 10991824},
  doi = {10.1002/we.338},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/we.338},
  urldate = {2021-11-11},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/QIDTKVHT/Kanev and van Engelen - 2010 - Wind turbine extreme gust control.pdf}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2021-10-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/65VICQXK/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/sph3re/.config/Zotero/storage/FXY2YT43/1412.html}
}

@techreport{kostLevelizedCostElectricity2021,
  title = {Levelized {{Cost}} of {{Electricity- Renewable Energy Technologies}}},
  author = {Kost, Christoph},
  year = {2021},
  month = jun,
  institution = {{Fraunhofer ISE}},
  url = {https://www.ise.fraunhofer.de/en/publications/studies/cost-of-electricity.html}
}

@article{kumarDR3ValueBasedDeep2021,
  title = {{{DR3}}: {{Value-Based Deep Reinforcement Learning Requires Explicit Regularization}}},
  shorttitle = {{{DR3}}},
  author = {Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville, Aaron and Tucker, George and Levine, Sergey},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.04716 [cs]},
  eprint = {2112.04716},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.04716},
  urldate = {2022-01-17},
  abstract = {Despite overparameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive "aliasing", in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/K7ZDTCX4/Kumar et al. - 2021 - DR3 Value-Based Deep Reinforcement Learning Requi.pdf;/home/sph3re/.config/Zotero/storage/PMHNPVZY/2112.html}
}

@article{lacknerComparisonSmartRotor2010,
  title = {A Comparison of Smart Rotor Control Approaches Using Trailing Edge Flaps and Individual Pitch Control},
  author = {Lackner, Matthew A. and {van Kuik}, Gijs},
  year = {2010},
  month = mar,
  journal = {Wind Energy},
  volume = {13},
  number = {2-3},
  pages = {117--134},
  issn = {10954244, 10991824},
  doi = {10.1002/we.353},
  url = {http://doi.wiley.com/10.1002/we.353},
  urldate = {2020-12-29},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/Q9LDAABR/Lackner and van Kuik - 2010 - A comparison of smart rotor control approaches usi.pdf}
}

@article{larsenActiveLoadReduction2005,
  title = {Active Load Reduction Using Individual Pitch, Based on Local Blade Flow Measurements},
  author = {Larsen, Torben Juul and Madsen, Helge A. and Thomsen, Kenneth},
  year = {2005},
  month = jan,
  journal = {Wind Energy},
  volume = {8},
  number = {1},
  pages = {67--80},
  issn = {1095-4244, 1099-1824},
  doi = {10.1002/we.141},
  url = {http://doi.wiley.com/10.1002/we.141},
  urldate = {2020-12-29},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/X2WBACDJ/Larsen et al. - 2005 - Active load reduction using individual pitch, base.pdf}
}

@article{leeAlgorithmAutonomousPowerincrease2020,
  title = {Algorithm for Autonomous Power-Increase Operation Using Deep Reinforcement Learning and a Rule-Based System},
  author = {Lee, Daeil and Arigi, Awwal Mohammed and Kim, Jonghyun},
  year = {2020},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {8},
  pages = {196727--196746},
  doi = {10.1109/ACCESS.2020.3034218}
}

@article{leitheadAlleviationUnbalancedRotor2009,
  title = {Alleviation of Unbalanced Rotor Loads by Single Blade Controllers},
  author = {Leithead, WE and Neilson, Victoria and Dominguez, S},
  year = {2009},
  publisher = {{Curran Associates, Inc.}}
}

@article{levineReinforcementLearningControl2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  year = {2018},
  month = may,
  journal = {arXiv:1805.00909 [cs, stat]},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.00909},
  urldate = {2021-11-27},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/CY7LY6LZ/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf;/home/sph3re/.config/Zotero/storage/KIJ5NMZ4/1805.html}
}

@article{liawTuneResearchPlatform2018,
  title = {Tune: {{A}} Research Platform for Distributed Model Selection and Training},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.05118},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{liComparativeAnalysisRegression2001,
  title = {Comparative {{Analysis}} of {{Regression}} and {{Artificial Neural Network Models}} for {{Wind Turbine Power Curve Estimation}}},
  author = {Li, Shuhui and Wunsch, Donald C. and O'Hair, Edgar and Giesselmann, Michael G.},
  year = {2001},
  month = nov,
  journal = {Journal of Solar Energy Engineering},
  volume = {123},
  number = {4},
  pages = {327--332},
  issn = {0199-6231, 1528-8986},
  doi = {10.1115/1.1413216},
  url = {https://asmedigitalcollection.asme.org/solarenergyengineering/article/123/4/327/461480/Comparative-Analysis-of-Regression-and-Artificial},
  urldate = {2020-06-30},
  abstract = {This paper examines and compares regression and artificial neural network models used for the estimation of wind turbine power curves. First, characteristics of wind turbine power generation are investigated. Then, models for turbine power curve estimation using both regression and neural network methods are presented and compared. The parameter estimates for the regression model and training of the neural network are completed with the wind farm data, and the performances of the two models are studied. The regression model is shown to be function dependent, and the neural network model obtains its power curve estimation through learning. The neural network model is found to possess better performance than the regression model for turbine power curve estimation under complicated influence factors.},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/D7YDNY9K/Li et al. - 2001 - Comparative Analysis of Regression and Artificial .pdf}
}

@article{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  journal = {arXiv:1509.02971 [cs, stat]},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2021-10-22},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/7STR5HD9/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf;/home/sph3re/.config/Zotero/storage/PBEHI9RH/1509.html}
}

@article{lioFundamentalPerformanceSimilarities2017,
  title = {Fundamental Performance Similarities between Individual Pitch Control Strategies for Wind Turbines},
  author = {Lio, Wai Hou and Jones, Bryn Ll. and Lu, Qian and Rossiter, J.A.},
  year = {2017},
  month = jan,
  journal = {International Journal of Control},
  volume = {90},
  number = {1},
  pages = {37--52},
  issn = {0020-7179, 1366-5820},
  doi = {10.1080/00207179.2015.1078912},
  url = {https://www.tandfonline.com/doi/full/10.1080/00207179.2015.1078912},
  urldate = {2021-12-06},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/ZWQDXLCK/Lio et al. - 2017 - Fundamental performance similarities between indiv.pdf}
}

@article{luAnalysisDesignColeman2015,
  title = {Analysis and Design of {{Coleman}} Transform-Based Individual Pitch Controllers for Wind-Turbine Load Reduction: {{Individual}} Blade-Pitch Control},
  shorttitle = {Analysis and Design of {{Coleman}} Transform-Based Individual Pitch Controllers for Wind-Turbine Load Reduction},
  author = {Lu, Q. and Bowyer, R. and Jones, B.Ll.},
  year = {2015},
  month = aug,
  journal = {Wind Energy},
  volume = {18},
  number = {8},
  pages = {1451--1468},
  issn = {10954244},
  doi = {10.1002/we.1769},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/we.1769},
  urldate = {2021-11-09},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/G5NS2V8G/Lu et al. - 2015 - Analysis and design of Coleman transform-based ind.pdf}
}

@article{martenQBladeModernTool2020,
  title = {{{QBlade}}: A Modern Tool for the Aeroelastic Simulation of Wind Turbines},
  shorttitle = {{{QBlade}}},
  author = {Marten, David},
  year = {2020},
  publisher = {{Technische Universit\"at Berlin}},
  doi = {10.14279/DEPOSITONCE-10646},
  url = {https://depositonce.tu-berlin.de/handle/11303/11758},
  urldate = {2021-10-21},
  abstract = {Windenergieanlagen sind gro\ss e und komplexe Maschinen, die \"uber ihre gesamte Lebensdauer unter hochgradig instation\"aren Randbedingungen betrieben werden. Um die Lebensdauer einer solchen Anlage vorherzusagen m\"ussen eine gro\ss e Anzahl von Simulationen durchgef\"uhrt werden, welche die stochastischen Eigenschaften des am Standort vorherrschenden Windes abbilden. Sowohl extreme Lasten, hervorgerufen durch Unwetter, St\"urme oder Erdbeben, als auch Dauerlasten, welche sich aus dem normalen Betrieb unter Einfluss einer turbulenten Wind Anstr\"omung ergeben, m\"ussen in den Berechnungen zum Lebensdauernachweis enthalten sein. Aus diesen Anforderungen ergibt sich, dass eine gro\ss e Anzahl an Simulationen ben\"otigt wird um statisch aussagekr\"aftige Daten zu erhalten. In der Regel werden Lastsimulationen mit Zeitschrittweiten durchgef\"uhrt die einem Rotorfortschritt zwischen 3\textdegree{} und 10\textdegree{} entsprechen. Dies f\"uhrt dazu, dass bei einer vollst\"andigen Lebensdauerberechnung nach dem IEC 61400-1 Standard insgesamt zwischen 10\^6 bis 10\^7 konvergierte aero-elastische Zeitschritte berechnet werden. Daraus l\"asst sich eine der zentralen Anforderungen an aero-elastische Simulationsmethoden f\"ur den Windenergiebereich ableiten: Die verwendeten Simulationsverfahren m\"ussen zwangsl\"aufig sehr effizient sein. Aeroelastische Simulationsmethoden setzen sich im Wesentlichen aus einem aerodynamischen- und einem strukturellen Simulationsverfahren zusammen. Diese Verfahren werden gekoppelt um die Auswirkungen des Zusammenspiels von aerodynamischen-, tr\"agheits-, gravitations- und elastischen Kr\"aften und Momenten zu berechnen. Als ein Teil der hier vorgestellten Arbeit wurde in den letzten 8 Jahren das aero-elastische Simulationstool QBlade entwickelt. Als Hauptziel hierbei galt es die Verwendung von neuen Aerodynamik- und Strukturmodellierungsmethoden zu erm\"oglichen, welche genauere und verl\"asslichere Ergebnisse liefern als die bisher in der Industrie verwendeten Verfahren. Generell lassen sich durch genauere Simulationsverfahren die Stromgestehungskosten senken da Sicherheitsfaktoren reduziert werden k\"onnen, Material eingespart wird und damit effizientere Designs erm\"oglicht werden. Im Vergleich mit den bisher eingesetzten Verfahren f\"uhren diese neuen Methoden allerdings zu einem gesteigerten Rechenbedarf. Die Verwendung dieser neuen Methoden im Auslegungs- und Zertifizierungskontext wird erst durch die konstante Steigerung der f\"ur den Endnutzer verf\"ugbaren Rechenleistung in die letzten Jahre erm\"oglicht. Mithilfe von massiver Parallelisierung durch high-end Garfikprozessoren (GPUs), sowie die Optimierung der neuen Berechnungsmethoden selbst, k\"onnen diese nun im gro\ss en Umfang eingesetzt werden. In dieser Arbeit werden die in QBlade verwendeten aerodynamischen und strukturellen Modelle vorgestellt, ihre Verwendung im Vergleich mit anderen Methoden begr\"undet sowie ihre Optimierung erl\"autert. Anhand einiger Beispiele wird weiterhin die Flexibilit\"at sowie das Anwendungsspektrum der hier entwickelten Simulationssoftware demonstriert.},
  collaborator = {Technische Universit{\"a}t Berlin and Technische Universit{\"a}t Berlin and Paschereit, Christian Oliver},
  langid = {english},
  keywords = {006 Spezielle Computerverfahren,621 Angewandte Physik,aeroelasticity,Aeroelastizitt,numerical simulation,numerische Simulation,QBlade,vortex methods,wind energy,wind turbine,Windenergie,Windenergieanlage,Wirbelverfahren}
}

@article{martenQBLADEOpenSource2013,
  title = {{{QBLADE}}: {{An}} Open Source Tool for Design and Simulation of Horizontal and Vertical Axis Wind Turbines},
  shorttitle = {{{QBlade}}},
  author = {Marten, David and Peukert, Juliane and Pechlivanoglou, Georgios and Nayeri, Christian and Paschereit, Christian},
  year = {2013},
  month = mar,
  journal = {International Journal of Emerging Technology and Advanced Engineering},
  volume = {3},
  pages = {264--269}
}

@inproceedings{matsuichiFatigueMetalsSubjected1968,
  title = {Fatigue of Metals Subjected to Varying Stress},
  author = {Matsuichi, M. and Endo, Tsutomu},
  year = {1968}
}

@article{meloConvergenceQlearningSimple2001,
  title = {Convergence of {{Q-learning}}: {{A}} Simple Proof},
  author = {Melo, Francisco S},
  year = {2001},
  journal = {Institute Of Systems and Robotics, Tech. Rep},
  pages = {1--4}
}

@article{minerCumulativeDamageFatigue1945,
  title = {Cumulative {{Damage}} in {{Fatigue}}},
  author = {Miner, Milton A.},
  year = {1945},
  month = sep,
  journal = {Journal of Applied Mechanics},
  volume = {12},
  number = {3},
  pages = {A159-A164},
  issn = {0021-8936, 1528-9036},
  doi = {10.1115/1.4009458},
  url = {https://asmedigitalcollection.asme.org/appliedmechanics/article/12/3/A159/1103202/Cumulative-Damage-in-Fatigue},
  urldate = {2022-02-19},
  abstract = {Abstract             The phenomenon of cumulative damage under repeated loads was assumed to be related to the net work absorbed by a specimen. The number of loading cycles applied expressed as a percentage of the number to failure at a given stress level would be the proportion of useful life expended. When the total damage, as defined by this concept, reached 100 per cent, the fatigue specimen should fail. Experimental verification of this concept for an aluminum alloy, using different types of specimens, various stress ratios, and various combinations of loading cycles is presented. These data are also analyzed to provide information on different stress ratios when an S-N curve for any one ratio is known. Results of a sample analysis based on experiments are given. It is concluded that a simple and conservative analysis is possible using the concept of cumulative fatigue damage.},
  langid = {english}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  url = {http://www.nature.com/articles/nature14236},
  urldate = {2021-11-03},
  langid = {english}
}

@article{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.5602 [cs]},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2021-10-22},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/Q3TZEINI/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/sph3re/.config/Zotero/storage/T2856M59/1312.html}
}

@misc{mordvintsevDeepDream2015,
  title = {Deep {{Dream}}},
  author = {Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
  year = {2015},
  month = jun,
  url = {https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
  urldate = {2022-02-08}
}

@article{mysoreRegularizingActionPolicies2021,
  title = {Regularizing {{Action Policies}} for {{Smooth Control}} with {{Reinforcement Learning}}},
  author = {Mysore, Siddharth and Mabsout, Bassel and Mancuso, Renato and Saenko, Kate},
  year = {2021},
  month = may,
  journal = {arXiv:2012.06644 [cs, eess]},
  eprint = {2012.06644},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2012.06644},
  urldate = {2021-10-18},
  abstract = {A critical problem with the practical utility of controllers trained with deep Reinforcement Learning (RL) is the notable lack of smoothness in the actions learned by the RL policies. This trend often presents itself in the form of control signal oscillation and can result in poor control, high power consumption, and undue system wear. We introduce Conditioning for Action Policy Smoothness (CAPS), an effective yet intuitive regularization on action policies, which offers consistent improvement in the smoothness of the learned state-to-action mappings of neural network controllers, reflected in the elimination of high-frequency components in the control signal. Tested on a real system, improvements in controller smoothness on a quadrotor drone resulted in an almost 80\% reduction in power consumption while consistently training flight-worthy controllers. Project website: http://ai.bu.edu/caps},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/sph3re/.config/Zotero/storage/AIXP3B6Q/Mysore et al. - 2021 - Regularizing Action Policies for Smooth Control wi.pdf;/home/sph3re/.config/Zotero/storage/VKCNE2GA/2012.html}
}

@article{navalkarSubspacePredictiveRepetitive2014,
  title = {Subspace Predictive Repetitive Control to Mitigate Periodic Loads on Large Scale Wind Turbines},
  author = {Navalkar, S.T. and {van Wingerden}, J.W. and {van Solingen}, E. and Oomen, T. and Pasterkamp, E. and {van Kuik}, G.A.M.},
  year = {2014},
  month = dec,
  journal = {Mechatronics},
  volume = {24},
  number = {8},
  pages = {916--925},
  issn = {09574158},
  doi = {10.1016/j.mechatronics.2014.01.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957415814000063},
  urldate = {2021-11-11},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/NWQS5BUS/Navalkar et al. - 2014 - Subspace predictive repetitive control to mitigate.pdf}
}

@misc{neilkelleyandbonniejonkmanTurbSim,
  title = {{{TurbSim}}},
  author = {{Neil Kelley {and} Bonnie Jonkman}},
  publisher = {{NREL}},
  url = {https://www.nrel.gov/wind/nwtc/turbsim.html},
  urldate = {2021-11-05},
  abstract = {TurbSim is a stochastic, full-field, turbulence simulator primarily for use with InflowWind-based simulation tools.}
}

@misc{openclipartWindTurbineSketch2017,
  title = {Wind Turbine Sketch},
  author = {OpenClipart},
  year = {2017},
  month = aug,
  url = {https://freesvg.org/wind-turbine-sketch},
  urldate = {2022-02-02}
}

@article{padullaparthiFALCONFArmLevel2022,
  title = {{{FALCON- FArm Level CONtrol}} for Wind Turbines Using Multi-Agent Deep Reinforcement Learning},
  author = {Padullaparthi, Venkata Ramakrishna and Nagarathinam, Srinarayana and Vasan, Arunchandar and Menon, Vishnu and Sudarsanam, Depak},
  year = {2022},
  month = jan,
  journal = {Renewable Energy},
  volume = {181},
  pages = {445--456},
  issn = {09601481},
  doi = {10.1016/j.renene.2021.09.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960148121013227},
  urldate = {2021-11-08},
  langid = {english}
}

@article{pearlCausalInferenceStatistics2009,
  title = {Causal Inference in Statistics: {{An}} Overview},
  author = {Pearl, Judea},
  year = {2009},
  journal = {Statistics surveys},
  volume = {3},
  pages = {96--146},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the \ldots}}
}

@article{perez-beckerActiveFlapControl2021,
  title = {Active Flap Control with the Trailing Edge Flap Hinge Moment as a Sensor: Using It to Estimate Local Blade Inflow Conditions and to Reduce Extreme Blade Loads and Deflections},
  shorttitle = {Active Flap Control with the Trailing Edge Flap Hinge Moment as a Sensor},
  author = {{Perez-Becker}, Sebastian and Marten, David and Paschereit, Christian Oliver},
  year = {2021},
  month = jun,
  journal = {Wind Energy Science},
  volume = {6},
  number = {3},
  pages = {791--814},
  issn = {2366-7451},
  doi = {10.5194/wes-6-791-2021},
  url = {https://wes.copernicus.org/articles/6/791/2021/},
  urldate = {2021-11-15},
  abstract = {Abstract. Active trailing edge flaps are a promising technology that can potentially enable further increases in wind turbine sizes without the disproportionate increase in loads, thus reducing the cost of wind energy even further. Extreme loads and critical deflections of the blade are design-driving issues that can effectively be reduced by flaps. In this paper, we consider the flap hinge moment as a local input sensor for a simple flap controller that reduces extreme loads and critical deflections of the DTU 10\,MW Reference Wind Turbine blade. We present a model to calculate the unsteady flap hinge moment that can be used in aeroelastic simulations in the time domain. This model is used to develop an observer that estimates the local angle of attack and relative wind velocity of a blade section based on local sensor information including the flap hinge moment of the blade section. For steady wind conditions that include yawed inflow and wind shear, the observer is able to estimate the local inflow conditions with errors in the mean angle of attack below 0.2{$\circ$} and mean relative wind speed errors below 0.4\,\%. For fully turbulent wind conditions, the observer is able to estimate the low-frequency content of the local angle of attack and relative velocity even when it is lacking information on the incoming turbulent wind. We include this observer as part of a simple flap controller to reduce extreme loads and critical deflections of the blade. The flap controller's performance is tested in load simulations of the reference turbine with active flaps according to the IEC~61400-1 power production with extreme turbulence group. We used the lifting line free vortex wake method to calculate the aerodynamic loads. Results show a reduction of the maximum out-of-plane and resulting blade root bending moments of 8\,\% and 7.6\,\%, respectively, when compared to a baseline case without flaps. The critical blade tip deflection is reduced by 7.1\,\%. Furthermore, a sector load analysis considering extreme loading in all load directions shows a reduction of the extreme resulting bending moment in an angular region covering 30{$\circ$} around the positive out-of-plane blade root bending moment. Further analysis reveals that a fast reaction time of the flap system proves to be critical for its performance. This is achieved with the use of local sensors as input for the flap controller. A larger reduction potential of the system is identified but not reached mainly because of a combination of challenging controller objectives and the simple controller architecture.},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/KWS8E7PL/Perez-Becker et al. - 2021 - Active flap control with the trailing edge flap hi.pdf}
}

@article{perez-beckerAdvancedAerodynamicModeling2021,
  title = {Advanced Aerodynamic Modeling and Control Strategies for Load Reduction in Aeroelastic Wind Turbine Simulations},
  author = {{Perez-Becker}, Sebastian Rafael},
  year = {2021},
  publisher = {{Technische Universit\"at Berlin}},
  doi = {10.14279/DEPOSITONCE-12726},
  url = {https://depositonce.tu-berlin.de/handle/11303/13952},
  urldate = {2022-02-19},
  abstract = {Die Vergr\"o\ss erung von Windturbinen ist einer der effektivsten Wege die Stromgestehungskosten von Windenergie zu verringern. Um diesen Trend zu erhalten muss gew\"ahrleistet werden, dass die designtreibenden Lasten der Turbinenkomponenten nicht \"uberproportional mit der Turbinengr\"o\ss e steigen. Eine versprechende Methode um dies zu erlangen ist die Verbesserung der aeroelastischen Modellierung in Windturbinensimulationen. Da aeroelastische Lastrechnungen eine kritische Rolle beim Design von Windturbinenkomponenten spielen, f\"uhrt eine Verbesserung der Modellierung zu genaueren Designlasten und weniger konservativen Sicherheitsfaktoren bei den Lasten. Eine andere versprechende Methode um die Designlasten von gro\ss en Windturbinen zu verringern ist die Anwendung von fortgeschrittenen lastreduzierenden Regelungsstrategien. Unterschiedliche Lasten beeinflussen die Turbinenkomponenten anders und deshalb m\"ussen auch verschiedene Aktuatoren in Betracht gezogen werden, um die desingtreibenden Lasten mehrerer Komponenten zu reduzieren. Diese Dissertation tr\"agt zu diesem Thema in zwei separaten aber sich erg\"anzenden Themenbereiche bei. Einerseits beinhaltet die Arbeit eine Studie zu fortgeschrittenen aerodynamischen Modellen und deren quantitativen Effekt auf die Turbinenlasten. Andererseits pr\"asentiert die Arbeit einen Windturbinenregler, der komplette Lastberechnungen durchf\"uhren kann und \"uber fortgeschrittene lastreduzierende Regelungsstrategien verf\"ugt. Diese Strategien nutzen verschiedene Aktuatoren wie Generator, Pitchaktuatoren und aktive Hinterkantenklappen und werden in unterschiedlichen Studien f\"ur die Verringerung von designtreibenden Extrem- und Erm\"udungslasten verwendet. Die Arbeit basiert auf drei Ver\"offentlichungen die verschiedene Wege zur Lastreduzierung untersuchen. Die erste Ver\"offentlichung beschreibt den Windturbinenregler im Detail. Die Ver\"offentlichung beschreibt auch die fortgeschrittenen Lastreduktionsstrategien des entwickelten Reglers, welche auf traditionellen Aktuatoren wie die Pitchaktuatoren und dem Generatormoment beruhen. Die Lastreduktionsf\"ahigkeit der Individual Pitch Control Strategie wird in aeroelastischen Simulationen untersucht, die das fortgeschrittene aerodynamische Modell Lifting Line Free Vortex Wake nutzen. Die zweite Ver\"offentlichung verwendet den Regler in einer Studie, die die Lasteffekte von fortgeschrittenen aerodynamischen Methoden in aeroelastischen Simulationen untersucht. Die Studie vergleicht Extrem- und Erm\"udungslasten von wichtigen Turbinensensoren, die in aeroelastischen Simulationen mit zwei verschieden aerodynamischen Methoden berechnet worden sind. Die erste Methode ist die verbreitete Blade Element Momentum Methode. Die zweite Methode ist die fortgeschrittene Lifting Line Free Vortex Wake Methode. Die Studie quantifiziert nicht nur die Lastunterschiede sondern findet auch die Ursachen dieser Unterschiede. Die dritte Ver\"offentlichung untersucht das Potential von aktiven Hinterkantenklappen um designtreibende Extremlasten und kritische Durchbiegungen vom Rotorblatt zu reduzieren. Sie betrachtet das Klappengelenk-Moment als ein m\"oglicher Regler-Sensor, der robust und bereits im Klappensystem vorhanden ist. In der Ver\"offentlichung wird das Klappengelenk-Moment und andere Sensoren f\"ur einen modellbasierten Beobachter genutzt, der die aerodynamischen Zust\"ande an der Blattsektion sch\"atzen kann. Dies erm\"oglicht eine schnelle Reaktion des Klappenreglers. Der Beobachter wird als Teil einer neuen Klappenstrategie verwendet, welche die Extremlasten und kritische Durchbiegung vom Rotorblatt effektiv reduziert. Die Ver\"offentlichungen untersuchen jeweils ein m\"oglicher Lastreduktionsaspekt f\"ur gr\"o\ss ere Windturbinen. Diese Dissertation enth\"alt Vorschl\"age, wie man diese einzelnen Aspekte verbessern kann. Alle Aspekte im Turbinen-Designprozess zu verwenden ist eine versprechende Option um die Kosten der Windenergie weiter zu reduzieren und die Nutzung dieser Technologie zu verbreiten.},
  collaborator = {Technische Universit{\"a}t Berlin and Technische Universit{\"a}t Berlin and Paschereit, Christian Oliver},
  langid = {english},
  keywords = {620 Ingenieurwissenschaften und zugeordnete Ttigkeiten,active trailing edge flap,aeroelastic simulation,aeroelastische Simulation,aktive Hinterkantenklappe,Lastreduktionsregelung,load reduction control,wind energy,wind turbine control,Windenergie,Windturbinenregelung}
}

@article{perez-beckerImplementationValidationAdvanced2021,
  title = {Implementation and {{Validation}} of an {{Advanced Wind Energy Controller}} in {{Aero-Servo-Elastic Simulations Using}} the {{Lifting Line Free Vortex Wake Model}}},
  author = {{Perez-Becker}, Sebastian and Marten, David and Nayeri, Christian Navid and Paschereit, Christian Oliver},
  year = {2021},
  month = feb,
  journal = {Energies},
  volume = {14},
  number = {3},
  pages = {783},
  issn = {1996-1073},
  doi = {10.3390/en14030783},
  url = {https://www.mdpi.com/1996-1073/14/3/783},
  urldate = {2021-11-15},
  abstract = {Accurate and reproducible aeroelastic load calculations are indispensable for designing modern multi-MW wind turbines. They are also essential for assessing the load reduction capabilities of advanced wind turbine control strategies. In this paper, we contribute to this topic by introducing the TUB Controller, an advanced open-source wind turbine controller capable of performing full load calculations. It is compatible with the aeroelastic software QBlade, which features a lifting line free vortex wake aerodynamic model. The paper describes in detail the controller and includes a validation study against an established open-source controller from the literature. Both controllers show comparable performance with our chosen metrics. Furthermore, we analyze the advanced load reduction capabilities of the individual pitch control strategy included in the TUB Controller. Turbulent wind simulations with the DTU 10 MW Reference Wind Turbine featuring the individual pitch control strategy show a decrease in the out-of-plane and torsional blade root bending moment fatigue loads of 14\% and 9.4\% respectively compared to a baseline controller.},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/E4CL5QGP/Perez-Becker et al. - 2021 - Implementation and Validation of an Advanced Wind .pdf}
}

@article{petrovicMPCFrameworkConstrained2021,
  title = {{{MPC}} Framework for Constrained Wind Turbine Individual Pitch Control},
  author = {Petrovi{\'c}, Vlaho and Jelavi{\'c}, Mate and Baoti{\'c}, Mato},
  year = {2021},
  month = jan,
  journal = {Wind Energy},
  volume = {24},
  number = {1},
  pages = {54--68},
  issn = {1095-4244, 1099-1824},
  doi = {10.1002/we.2558},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/we.2558},
  urldate = {2022-02-18},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/35666KN4/Petrovi et al. - 2021 - MPC framework for constrained wind turbine individ.pdf}
}

@article{piLowlevelAutonomousControl2020,
  title = {Low-Level Autonomous Control and Tracking of Quadrotor Using Reinforcement Learning},
  author = {Pi, Chen-Huan and Hu, Kai-Chun and Cheng, Stone and Wu, I-Chen},
  year = {2020},
  month = feb,
  journal = {Control Engineering Practice},
  volume = {95},
  pages = {104222},
  issn = {09670661},
  doi = {10.1016/j.conengprac.2019.104222},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0967066119301923},
  urldate = {2021-11-08},
  langid = {english}
}

@article{qinDifferentialEvolutionAlgorithm2009,
  title = {Differential {{Evolution Algorithm With Strategy Adaptation}} for {{Global Numerical Optimization}}},
  author = {Qin, A.K. and Huang, V.L. and Suganthan, P.N.},
  year = {2009},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {13},
  number = {2},
  pages = {398--417},
  issn = {1941-0026, 1089-778X},
  doi = {10.1109/TEVC.2008.927706},
  url = {http://ieeexplore.ieee.org/document/4632146/},
  urldate = {2020-06-30},
  file = {/home/sph3re/Programming/inf-pm-anw/literature/qin2009.pdf}
}

@article{raffinSmoothExplorationRobotic2021,
  title = {Smooth {{Exploration}} for {{Robotic Reinforcement Learning}}},
  author = {Raffin, Antonin and Kober, Jens and Stulp, Freek},
  year = {2021},
  month = jun,
  journal = {arXiv:2005.05719 [cs, stat]},
  eprint = {2005.05719},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.05719},
  urldate = {2021-12-21},
  abstract = {Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/L8K6ECPX/Raffin et al. - 2021 - Smooth Exploration for Robotic Reinforcement Learn.pdf;/home/sph3re/.config/Zotero/storage/5ABXNNQH/2005.html}
}

@incollection{raghebWindTurbinesTheory2011,
  title = {Wind {{Turbines Theory}} - {{The Betz Equation}} and {{Optimal Rotor Tip Speed Ratio}}},
  booktitle = {Fundamental and {{Advanced Topics}} in {{Wind Power}}},
  author = {Ragheb, Magdi and M., Adam},
  editor = {Carriveau, Rupp},
  year = {2011},
  month = jun,
  publisher = {{InTech}},
  doi = {10.5772/21398},
  url = {http://www.intechopen.com/books/fundamental-and-advanced-topics-in-wind-power/wind-turbines-theory-the-betz-equation-and-optimal-rotor-tip-speed-ratio},
  urldate = {2021-11-30},
  isbn = {978-953-307-508-2},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/FDDQYMGH/Ragheb and M. - 2011 - Wind Turbines Theory - The Betz Equation and Optim.pdf}
}

@article{rechtTourReinforcementLearning2019,
  title = {A {{Tour}} of {{Reinforcement Learning}}: {{The View}} from {{Continuous Control}}},
  shorttitle = {A {{Tour}} of {{Reinforcement Learning}}},
  author = {Recht, Benjamin},
  year = {2019},
  month = may,
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {2},
  number = {1},
  pages = {253--279},
  issn = {2573-5144, 2573-5144},
  doi = {10.1146/annurev-control-053018-023825},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-control-053018-023825},
  urldate = {2020-06-30},
  abstract = {This article surveys reinforcement learning from the perspective of optimization and control, with a focus on continuous control applications. It reviews the general formulation, terminology, and typical experimental implementations of reinforcement learning as well as competing solution paradigms. In order to compare the relative merits of various techniques, it presents a case study of the linear quadratic regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. It also describes how merging techniques from learning theory and control can provide nonasymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. The article concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/2REERENG/Recht - 2019 - A Tour of Reinforcement Learning The View from Co.pdf;/home/sph3re/Programming/inf-pm-anw/literature/A Tour of Reinforcement Learning.md}
}

@article{reddyGliderSoaringReinforcement2018,
  title = {Glider Soaring via Reinforcement Learning in the Field},
  author = {Reddy, Gautam and {Wong-Ng}, Jerome and Celani, Antonio and Sejnowski, Terrence J. and Vergassola, Massimo},
  year = {2018},
  month = oct,
  journal = {Nature},
  volume = {562},
  number = {7726},
  pages = {236--239},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0533-0},
  url = {http://www.nature.com/articles/s41586-018-0533-0},
  urldate = {2021-11-08},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/XUHUEL9I/Reddy et al. - 2018 - Glider soaring via reinforcement learning in the f.pdf}
}

@article{renImprovingGeneralizationReinforcement2020,
  title = {Improving {{Generalization}} of {{Reinforcement Learning}} with {{Minimax Distributional Soft Actor-Critic}}},
  author = {Ren, Yangang and Duan, Jingliang and Guan, Yang and Li, Shengbo Eben},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05502 [cs, stat]},
  eprint = {2002.05502},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.05502},
  urldate = {2020-07-19},
  abstract = {Reinforcement learning (RL) has achieved remarkable performance in a variety of sequential decision making and control tasks. However, a common problem is that learned nearly optimal policy always overfits to the training environment and may not be extended to situations never encountered during training. For practical applications, the randomness of the environment usually leads to rare but devastating events, which should be the focus of safety-critical systems, such as autonomous driving. In this paper, we introduce the minimax formulation and distributional framework to improve the generalization ability of RL algorithms and develop the Minimax Distributional Soft Actor-Critic (Minimax DSAC) algorithm. Minimax formulation aims to seek optimal policy considering the most serious disturbances from environment, in which the protagonist policy maximizes action-value function while the adversary policy tries to minimize it. Distributional framework aims to learn a state-action return distribution, from which we can model the risk of different returns explicitly, thus, formulating a risk-averse protagonist policy and a risk-seeking adversarial policy. We implement our method on the decision-making tasks of autonomous vehicles at intersections and test the trained policy in distinct environments from training environment. Results demonstrate that our method can greatly improve the generalization ability of the protagonist agent to different environmental variations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/M6J5BZV9/Ren et al. - 2020 - Improving Generalization of Reinforcement Learning.pdf;/home/sph3re/.config/Zotero/storage/UGC8RFFH/2002.html}
}

@article{richardbellmanTheoryDynamicProgramming1954,
  title = {The Theory of Dynamic Programming},
  author = {{Richard Bellman}},
  year = {1954},
  month = nov,
  journal = {Bulletin of the American Mathematical Society},
  volume = {60},
  number = {6},
  pages = {503--515},
  url = {https://doi.org/}
}

@article{saenz-aguirreArtificialNeuralNetwork2019,
  title = {Artificial {{Neural Network Based Reinforcement Learning}} for {{Wind Turbine Yaw Control}}},
  author = {{Saenz-Aguirre}, Aitor and Zulueta, Ekaitz and {Fernandez-Gamiz}, Unai and Lozano, Javier and {Lopez-Guede}, Jose},
  year = {2019},
  month = jan,
  journal = {Energies},
  volume = {12},
  number = {3},
  pages = {436},
  issn = {1996-1073},
  doi = {10.3390/en12030436},
  url = {http://www.mdpi.com/1996-1073/12/3/436},
  urldate = {2021-11-08},
  abstract = {This paper introduces a novel data driven yaw control algorithm synthesis method based on Reinforcement Learning (RL) for a variable pitch variable speed wind turbine. Yaw control has not been extendedly studied in the literature; in fact, most of the currently considered developments in the scope of the wind energy are oriented to the pitch and speed control. The most important drawbacks of the yaw control are the very large time constants and the strict yaw angle change rate constraints due to the high mechanical loads when the wind turbine angle is changed in order to adequate it to the wind speed orientation. An optimal yaw control algorithm needs to be designed in order to adapt the rotor orientation depending on the wind turbine dynamics and the local wind speed regime. Consequently, the biggest challenge of the yaw control algorithm is to decide the moment and the quantity of the wind turbine orientation variation to achieve the highest quantity of power at each instant, taking into account the constraints derived from the mechanical limitations of the yawing system and the mechanical loads. In this paper, a novel based algorithm based on the RL Q-Learning algorithm is introduced. The first step is to obtain a model of the power generated by the wind turbine (a real onshore wind turbine in this paper) through a power curve, that in conjunction with a conventional proportional regulator will be used to obtain a dataset that explains the actual behaviour of the real wind turbine when a variety of different yaw control commands are imposed. That knowledge is then used to learn the best control action for each different state of the wind turbine with respect to the wind direction represented by the yaw angle, storing that knowledge in a matrix Q(s,a). The last step is to model that matrix through a MultiLayer Perceptron with BackPropagation (MLP-BP) Artificial Neural Network (ANN) to avoid large matrix management and quantification problems. Once that the optimal yaw controller has been synthetized, its performance has been assessed using a number of wind speed realizations obtained using the software application TurbSim, in order to analyze how the introduced novel algorithm deals with different wind speed scenarios.},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/DY6B6N9X/Saenz-Aguirre et al. - 2019 - Artificial Neural Network Based Reinforcement Lear.pdf}
}

@article{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  journal = {arXiv:1707.06347 [cs]},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2020-06-21},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/CF57TJVM/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/sph3re/.config/Zotero/storage/GGVAP2MW/1707.html}
}

@article{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2017},
  month = apr,
  journal = {arXiv:1502.05477 [cs]},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1502.05477},
  urldate = {2020-06-21},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/K5BMFY2P/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf;/home/sph3re/.config/Zotero/storage/W6XNJ74L/1502.html}
}

@article{shenDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} with {{Robust}} and {{Smooth Policy}}},
  author = {Shen, Qianli and Li, Yan and Jiang, Haoming and Wang, Zhaoran and Zhao, Tuo},
  year = {2020},
  month = aug,
  journal = {arXiv:2003.09534 [cs, stat]},
  eprint = {2003.09534},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.09534},
  urldate = {2021-11-03},
  abstract = {Deep reinforcement learning (RL) has achieved great empirical successes in various domains. However, the large search space of neural networks requires a large amount of data, which makes the current RL algorithms not sample efficient. Motivated by the fact that many environments with continuous state space have smooth transitions, we propose to learn a smooth policy that behaves smoothly with respect to states. We develop a new framework -- \textbackslash textbf\{S\}mooth \textbackslash textbf\{R\}egularized \textbackslash textbf\{R\}einforcement \textbackslash textbf\{L\}earning (\$\textbackslash textbf\{SR\}\^2\textbackslash textbf\{L\}\$), where the policy is trained with smoothness-inducing regularization. Such regularization effectively constrains the search space, and enforces smoothness in the learned policy. Moreover, our proposed framework can also improve the robustness of policy against measurement error in the state space, and can be naturally extended to distribubutionally robust setting. We apply the proposed framework to both on-policy (TRPO) and off-policy algorithm (DDPG). Through extensive experiments, we demonstrate that our method achieves improved sample efficiency and robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/LN7LUJGV/Shen et al. - 2020 - Deep Reinforcement Learning with Robust and Smooth.pdf;/home/sph3re/.config/Zotero/storage/Z69X8HMD/2003.html}
}

@inproceedings{silverDeterministicPolicyGradient2014,
  title = {Deterministic Policy Gradient Algorithms},
  booktitle = {{{ICML}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas Manfred Otto and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin A.},
  year = {2014}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  url = {http://www.nature.com/articles/nature16961},
  urldate = {2021-10-18},
  langid = {english}
}

@article{songGeneralizationTowerNetwork2018,
  title = {Generalization {{Tower Network}}: {{A Novel Deep Neural Network Architecture}} for {{Multi-Task Learning}}},
  shorttitle = {Generalization {{Tower Network}}},
  author = {Song, Yuhang and Xu, Main and Zhang, Songyang and Huo, Liangyu},
  year = {2018},
  month = jan,
  journal = {arXiv:1710.10036 [cs, stat]},
  eprint = {1710.10036},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.10036},
  urldate = {2020-07-18},
  abstract = {Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by incorporating deep neural networks in learning representations from the input to RL. However, the conventional deep neural network architecture is limited in learning representations for multi-task RL (MT-RL), as multiple tasks can refer to different kinds of representations. In this paper, we thus propose a novel deep neural network architecture, namely generalization tower network (GTN), which can achieve MT-RL within a single learned model. Specifically, the architecture of GTN is composed of both horizontal and vertical streams. In our GTN architecture, horizontal streams are used to learn representation shared in similar tasks. In contrast, the vertical streams are introduced to be more suitable for handling diverse tasks, which encodes hierarchical shared knowledge of these tasks. The effectiveness of the introduced vertical stream is validated by experimental results. Experimental results further verify that our GTN architecture is able to advance the state-of-the-art MT-RL, via being tested on 51 Atari games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/EW3PG7XM/Song et al. - 2018 - Generalization Tower Network A Novel Deep Neural .pdf;/home/sph3re/.config/Zotero/storage/WJV45WNT/1710.html}
}

@techreport{stehly2019CostWind2020,
  title = {2019 Cost of Wind Energy Review},
  author = {Stehly, Tyler and Beiter, Philipp and Duffy, Patrick},
  year = {2020},
  institution = {{National Renewable Energy Lab.(NREL), Golden, CO (United States)}}
}

@inproceedings{suttonPolicyGradientMethods2000,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  editor = {Solla, S. and Leen, T. and M{\"u}ller, K.},
  year = {2000},
  volume = {12},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning}
}

@inproceedings{tesslerActionRobustReinforcement2019,
  title = {Action Robust Reinforcement Learning and Applications in Continuous Control},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Tessler, Chen and Efroni, Yonathan and Mannor, Shie},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {6215--6224},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  url = {http://proceedings.mlr.press/v97/tessler19a.html},
  abstract = {A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Specifically, we consider two scenarios in which the agent attempts to perform an action , and (i) with probability {$\alpha$}, an alternative adversarial action{\= } is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.}
}

@inproceedings{tominIntelligentControlWind2019,
  title = {Intelligent {{Control}} of a {{Wind Turbine}} Based on {{Reinforcement Learning}}},
  booktitle = {2019 16th {{Conference}} on {{Electrical Machines}}, {{Drives}} and {{Power Systems}} ({{ELMA}})},
  author = {Tomin, Nikita and Kurbatsky, Victor and Guliyev, Huseyngulu},
  year = {2019},
  month = jun,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Varna, Bulgaria}},
  doi = {10.1109/ELMA.2019.8771645},
  url = {https://ieeexplore.ieee.org/document/8771645/},
  urldate = {2020-07-02},
  isbn = {978-1-72811-413-2},
  file = {/home/sph3re/Programming/inf-pm-anw/literature/tomin2019.pdf}
}

@article{trung-kienphamLqrControlMultiMw2012,
  title = {Lqr {{Control For A Multi-Mw Wind Turbine}}},
  author = {{Trung-Kien Pham} and Yoonsu Nam and Hyungun Kim and Jaehoon Son},
  year = {2012},
  month = feb,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.1062490},
  url = {https://zenodo.org/record/1062490},
  urldate = {2022-02-18},
  abstract = {This paper addresses linear quadratic regulation (LQR) for variable speed variable pitch wind turbines. Because of the inherent nonlinearity of wind turbine, a set of operating conditions is identified and then a LQR controller is designed for each operating point. The feedback controller gains are then interpolated linearly to get control law for the entire operating region. Besides, the aerodynamic torque and effective wind speed are estimated online to get the gain-scheduling variable for implementing the controller. The potential of the method is verified through simulation with the help of MATLAB/Simulink and GH Bladed. The performance and mechanical load when using LQR are also compared with that when using PI controller.},
  copyright = {Creative Commons Attribution 4.0, Open Access},
  langid = {english},
  keywords = {LQR control.,multi-MW size wind turbine,variable speed variable pitch wind turbine,wind energy conversion system}
}

@article{vansolingenLinearIndividualPitch2015,
  title = {Linear Individual Pitch Control Design for Two-Bladed Wind Turbines: {{Linear}} Individual Pitch Control Design for Two-Bladed Wind Turbines},
  shorttitle = {Linear Individual Pitch Control Design for Two-Bladed Wind Turbines},
  author = {{van Solingen}, E. and {van Wingerden}, J.W.},
  year = {2015},
  month = apr,
  journal = {Wind Energy},
  volume = {18},
  number = {4},
  pages = {677--697},
  issn = {10954244},
  doi = {10.1002/we.1720},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/we.1720},
  urldate = {2022-02-09},
  langid = {english}
}

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  url = {http://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2021-10-18},
  langid = {english}
}

@article{wangSampleEfficientActorCritic2017,
  title = {Sample {{Efficient Actor-Critic}} with {{Experience Replay}}},
  author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and {de Freitas}, Nando},
  year = {2017},
  month = jul,
  journal = {arXiv:1611.01224 [cs]},
  eprint = {1611.01224},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.01224},
  urldate = {2020-07-01},
  abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/FXFYT7N4/Wang et al. - 2017 - Sample Efficient Actor-Critic with Experience Repl.pdf;/home/sph3re/.config/Zotero/storage/MF5UWYEJ/1611.html}
}

@article{welfordNoteMethodCalculating1962,
  title = {Note on a {{Method}} for {{Calculating Corrected Sums}} of {{Squares}} and {{Products}}},
  author = {Welford, B. P.},
  year = {1962},
  month = aug,
  journal = {Technometrics},
  volume = {4},
  number = {3},
  pages = {419--420},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1962.10490022},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1962.10490022},
  urldate = {2021-12-01},
  langid = {english}
}

@misc{westerbeckFrameworkDistributedWind2021,
  title = {A Framework for Distributed Wind Turbine Reinforcement Learning},
  author = {Westerbeck, Nico},
  year = {2021},
  month = oct
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3-4},
  pages = {229--256},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00992696},
  url = {http://link.springer.com/10.1007/BF00992696},
  urldate = {2021-10-22},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/ATSBL8TL/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf}
}

@article{wuScalableTrustregionMethod2017,
  title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using {{Kronecker-factored}} Approximation},
  author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.05144 [cs]},
  eprint = {1708.05144},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1708.05144},
  urldate = {2020-06-21},
  abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/sph3re/.config/Zotero/storage/Q55LNTG4/Wu et al. - 2017 - Scalable trust-region method for deep reinforcemen.pdf;/home/sph3re/.config/Zotero/storage/UJTCTQEQ/1708.html}
}

@article{yangIndividualPitchController2016,
  title = {Individual Pitch Controller Based on Fuzzy Logic Control for Wind Turbine Load Mitigation},
  author = {Yang, Fan and Han, Bing and Xiang, Zeng and Zhou, Lawu},
  year = {2016},
  month = may,
  journal = {IET Renewable Power Generation},
  volume = {10},
  number = {5},
  pages = {687--693},
  issn = {1752-1416, 1752-1424},
  doi = {10.1049/iet-rpg.2015.0320},
  url = {https://digital-library.theiet.org/content/journals/10.1049/iet-rpg.2015.0320},
  urldate = {2020-06-30},
  langid = {english},
  file = {/home/sph3re/.config/Zotero/storage/F75D75TG/Yang et al. - 2016 - Individual pitch controller based on fuzzy logic c.pdf}
}

@article{yilmazPitchAngleControl2009,
  title = {Pitch Angle Control in Wind Turbines above the Rated Wind Speed by Multi-Layer Perceptron and Radial Basis Function Neural Networks},
  author = {Yilmaz, Ahmet Serdar and {\"O}zer, Zafer},
  year = {2009},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {36},
  number = {6},
  pages = {9767--9775},
  issn = {09574174},
  doi = {10.1016/j.eswa.2009.02.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S095741740900164X},
  urldate = {2020-06-30},
  langid = {english}
}

@article{zeilerVisualizingUnderstandingConvolutional2013,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2013},
  month = nov,
  journal = {arXiv:1311.2901 [cs]},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1311.2901},
  urldate = {2022-02-08},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/sph3re/.config/Zotero/storage/74NBVDHF/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf;/home/sph3re/.config/Zotero/storage/6Z63FHSX/1311.html}
}

@misc{ZeroMQ,
  title = {{{ZeroMQ}}},
  url = {https://github.com/zeromq/libzmq}
}

@article{zhangReinforcementLearningBasedStructural2020,
  title = {Reinforcement {{Learning-Based Structural Control}} of {{Floating Wind Turbines}}},
  author = {Zhang, Jincheng and Zhao, Xiaowei and Wei, Xing},
  year = {2020},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  pages = {1--11},
  issn = {2168-2216, 2168-2232},
  doi = {10.1109/TSMC.2020.3032622},
  url = {https://ieeexplore.ieee.org/document/9254086/},
  urldate = {2021-11-08},
  file = {/home/sph3re/.config/Zotero/storage/JUTH8J5E/Zhang et al. - 2020 - Reinforcement Learning-Based Structural Control of.pdf}
}

@article{zhaoCooperativeWindFarm2020,
  title = {Cooperative Wind Farm Control with Deep Reinforcement Learning and Knowledge-Assisted Learning},
  author = {Zhao, Huan and Zhao, Junhua and Qiu, Jing and Liang, Gaoqi and Dong, Zhao Yang},
  year = {2020},
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {16},
  number = {11},
  pages = {6912--6921},
  doi = {10.1109/TII.2020.2974037}
}

@inproceedings{ziebartMaximumEntropyInverse2008,
  title = {Maximum Entropy Inverse Reinforcement Learning},
  booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
  author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
  year = {2008},
  series = {{{AAAI}}'08},
  pages = {1433--1438},
  publisher = {{AAAI Press}},
  address = {{Chicago, Illinois}},
  abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
  isbn = {978-1-57735-368-3}
}


